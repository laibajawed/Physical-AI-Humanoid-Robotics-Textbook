# Implementation Plan: Embedding Pipeline Setup

**Branch**: `003-embedding-pipeline` | **Date**: 2025-12-15 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/003-embedding-pipeline/spec.md`

## Summary

Build a single-file embedding pipeline (`backend/main.py`) that:
1. Discovers and fetches documentation URLs from the deployed Docusaurus book
2. Extracts and chunks text content with markdown-aware splitting
3. Generates embeddings using Cohere `embed-english-v3.0` (1024 dimensions)
4. Stores vectors in Qdrant Cloud collection `rag_embedding` with full metadata
5. Supports idempotent re-runs via deterministic IDs and content hashing

## Technical Context

**Language/Version**: Python 3.11
**Package Manager**: UV (https://github.com/astral-sh/uv)
**Primary Dependencies**:
- `cohere` (>=4.30) - Embedding generation
- `qdrant-client` - Vector storage
- `httpx` - Async HTTP client for URL fetching
- `beautifulsoup4` + `lxml` - HTML parsing and text extraction
- `python-dotenv` - Environment variable management

**Storage**: Qdrant Cloud free tier (1GB RAM, 4GB disk)
**Testing**: pytest with httpx mock
**Target Platform**: Python CLI script (local execution)
**Project Type**: Single file (`main.py`)
**Performance Goals**: Full book ingestion (13+ docs) in under 10 minutes
**Constraints**: Cohere trial API (100 calls/min, 96 texts/batch max)
**Scale/Scope**: ~13-16 documentation URLs, ~100-500 chunks total

## Constitution Check

| Principle | Status | Notes |
|-----------|--------|-------|
| CODE QUALITY | âœ… Pass | Python with type hints, single-file design |
| USER EXPERIENCE | âœ… Pass | CLI with JSON structured logging |
| CONTENT ORGANIZATION | âœ… Pass | Processes book content by module structure |
| DESIGN STANDARDS | âœ… Pass | Minimal dependencies, clean function design |

## Project Structure

### Documentation (this feature)

```text
specs/003-embedding-pipeline/
â”œâ”€â”€ spec.md              # Feature specification
â”œâ”€â”€ plan.md              # This file
â”œâ”€â”€ checklists/
â”‚   â””â”€â”€ requirements.md  # Requirements checklist
â””â”€â”€ tasks.md             # Tasks (generated by /sp.tasks)
```

### Source Code (repository root)

```text
backend/
â”œâ”€â”€ main.py              # Single-file embedding pipeline (ALL functions here)
â”œâ”€â”€ pyproject.toml       # UV project configuration
â”œâ”€â”€ .python-version      # Python version (3.11)
â”œâ”€â”€ .env.example         # Environment variable template
â”œâ”€â”€ .env                  # Local environment variables (gitignored)
â””â”€â”€ tests/
    â””â”€â”€ test_main.py     # Unit tests for pipeline functions
```

**Structure Decision**: Single-file architecture as requested. All six core functions (`get_all_urls`, `extract_text_from_url`, `chunk_text`, `embed`, `create_collection`, `save_chunk_to_qdrant`) plus `main()` reside in `backend/main.py`.

---

## UV Project Initialization

### Setup Commands

```bash
# 1. Create backend directory
mkdir -p backend

# 2. Navigate to backend
cd backend

# 3. Initialize UV project
uv init

# 4. Set Python version
echo "3.11" > .python-version

# 5. Add dependencies
uv add cohere qdrant-client httpx beautifulsoup4 lxml python-dotenv

# 6. Add dev dependencies
uv add --dev pytest pytest-asyncio

# 7. Create virtual environment and sync
uv sync
```

### pyproject.toml (Generated by UV)

```toml
[project]
name = "embedding-pipeline"
version = "0.1.0"
description = "RAG embedding pipeline for Docusaurus documentation"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "cohere>=4.30",
    "qdrant-client>=1.7.0",
    "httpx>=0.25.0",
    "beautifulsoup4>=4.12.0",
    "lxml>=5.0.0",
    "python-dotenv>=1.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
```

### Running the Pipeline

```bash
# Run with UV
uv run python main.py

# Or activate venv and run directly
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows
python main.py
```

---

## Architecture Design

### System Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           main() Entry Point                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Load environment variables (COHERE_API_KEY, QDRANT_URL, QDRANT_API_KEY) â”‚
â”‚  2. Initialize Cohere and Qdrant clients                                     â”‚
â”‚  3. create_collection("rag_embedding") - idempotent                          â”‚
â”‚  4. get_all_urls(base_url) â†’ list[str]                                       â”‚
â”‚  5. For each URL:                                                            â”‚
â”‚     a. extract_text_from_url(url) â†’ (title, section, text)                   â”‚
â”‚     b. chunk_text(text, url) â†’ list[Chunk]                                   â”‚
â”‚     c. embed(chunks) â†’ list[vector]  (batched, 96 max per call)              â”‚
â”‚     d. save_chunk_to_qdrant(chunks, vectors, metadata)                       â”‚
â”‚  6. Print execution report (JSON structured)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Function Specifications

#### 1. `get_all_urls(base_url: str) -> list[str]`

**Purpose**: Discover all documentation URLs from sitemap or fallback to hardcoded list.

**Input**: Base URL of deployed site (e.g., `https://hackathon-i-physical-ai-humanoid-ro-dun.vercel.app/`)

**Output**: Deduplicated list of `/docs/**` URLs

**Logic**:
```python
def get_all_urls(base_url: str) -> list[str]:
    # 1. Try fetching sitemap.xml
    sitemap_url = f"{base_url.rstrip('/')}/sitemap.xml"
    try:
        response = httpx.get(sitemap_url, timeout=30)
        if response.status_code == 200:
            # Parse XML, extract <loc> elements
            # Filter to /docs/** paths only
            # Exclude /search, /tags/*, /blog/*
            return deduplicated_urls
    except:
        pass

    # 2. Fallback to hardcoded URLs
    FALLBACK_URLS = [
        f"{base_url}/docs/introduction/",
        f"{base_url}/docs/module1-ros2-fundamentals/chapter1-ros2-basics",
        f"{base_url}/docs/module1-ros2-fundamentals/chapter2-ros2-navigation",
        f"{base_url}/docs/module1-ros2-fundamentals/chapter3-ros2-manipulation",
        f"{base_url}/docs/module2-simulation-gazebo-unity/chapter4-gazebo-simulation",
        f"{base_url}/docs/module2-simulation-gazebo-unity/chapter5-unity-simulation",
        f"{base_url}/docs/module2-simulation-gazebo-unity/chapter6-sim-environments",
        f"{base_url}/docs/module3-advanced-robotics-nvidia-isaac/chapter7-isaac-sdk",
        f"{base_url}/docs/module3-advanced-robotics-nvidia-isaac/chapter8-isaac-manipulation",
        f"{base_url}/docs/module4-vla-systems/chapter9-vision-language",
        f"{base_url}/docs/module4-vla-systems/chapter10-action-systems",
        f"{base_url}/docs/resources/",
        f"{base_url}/docs/conclusion/",
    ]
    return FALLBACK_URLS
```

---

#### 2. `extract_text_from_url(url: str) -> tuple[str, str, str]`

**Purpose**: Fetch HTML and extract clean text content from Docusaurus page.

**Input**: Full URL of documentation page

**Output**: Tuple of `(title, section, extracted_text)`

**Logic**:
```python
def extract_text_from_url(url: str) -> tuple[str, str, str]:
    # 1. Fetch HTML with 30s timeout
    response = httpx.get(url, timeout=30)
    response.raise_for_status()

    # 2. Parse with BeautifulSoup
    soup = BeautifulSoup(response.text, 'lxml')

    # 3. Extract title from <title> or <h1>
    title = soup.find('title').get_text(strip=True) if soup.find('title') else ""

    # 4. Extract section from URL path (e.g., "module1-ros2-fundamentals")
    section = extract_section_from_url(url)

    # 5. Extract main content using Docusaurus selectors (priority order):
    #    - article.markdown
    #    - main[class*="docMainContainer"]
    #    - div[class*="theme-doc-markdown"]
    #    - <main> or <article> fallback
    content_element = (
        soup.select_one('article.markdown') or
        soup.select_one('main[class*="docMainContainer"]') or
        soup.select_one('div[class*="theme-doc-markdown"]') or
        soup.find('main') or
        soup.find('article')
    )

    # 6. Extract text, preserve code blocks
    text = content_element.get_text(separator='\n', strip=True) if content_element else ""

    # 7. Normalize whitespace
    text = normalize_whitespace(text)

    # 8. Skip if < 100 characters
    if len(text) < 100:
        raise ValueError(f"Insufficient content: {len(text)} chars")

    return (title, section, text)
```

---

#### 3. `chunk_text(text: str, source_url: str) -> list[dict]`

**Purpose**: Split text into overlapping chunks suitable for embedding.

**Input**:
- `text`: Extracted document text
- `source_url`: URL for generating deterministic chunk IDs

**Output**: List of chunk dictionaries with `id`, `text`, `position`

**Logic**:
```python
def chunk_text(text: str, source_url: str) -> list[dict]:
    # Target: 350 tokens (~1400 chars), Overlap: 60 tokens (~240 chars)
    CHUNK_SIZE = 1400  # characters (approximate token proxy)
    CHUNK_OVERLAP = 240

    # Markdown-aware separators (priority order)
    SEPARATORS = ['\n## ', '\n### ', '\n#### ', '\n\n', '\n', ' ']

    chunks = []
    position = 0
    start = 0

    while start < len(text):
        end = start + CHUNK_SIZE

        # Find best split point using separators
        if end < len(text):
            for sep in SEPARATORS:
                split_pos = text.rfind(sep, start, end)
                if split_pos > start:
                    end = split_pos + len(sep)
                    break

        chunk_text = text[start:end].strip()

        if chunk_text:
            # Generate deterministic ID: SHA-256(source_url + position)[:32]
            chunk_id = hashlib.sha256(f"{source_url}:{position}".encode()).hexdigest()[:32]

            chunks.append({
                'id': chunk_id,
                'text': chunk_text,
                'position': position,
            })
            position += 1

        # Move start with overlap
        start = end - CHUNK_OVERLAP
        if start >= len(text):
            break

    return chunks
```

---

#### 4. `embed(texts: list[str]) -> list[list[float]]`

**Purpose**: Generate embeddings using Cohere API with batching.

**Input**: List of text strings to embed

**Output**: List of 1024-dimension embedding vectors

**Logic**:
```python
def embed(texts: list[str], cohere_client) -> list[list[float]]:
    BATCH_SIZE = 96  # Cohere max per call
    all_embeddings = []

    for i in range(0, len(texts), BATCH_SIZE):
        batch = texts[i:i + BATCH_SIZE]

        # Retry with exponential backoff
        for attempt in range(3):
            try:
                response = cohere_client.embed(
                    texts=batch,
                    model="embed-english-v3.0",
                    input_type="search_document"
                )
                all_embeddings.extend(response.embeddings)
                break
            except Exception as e:
                if attempt == 2:
                    raise
                time.sleep(2 ** attempt)  # 1s, 2s, 4s backoff

    return all_embeddings
```

---

#### 5. `create_collection(collection_name: str)`

**Purpose**: Create Qdrant collection if it doesn't exist, idempotent.

**Input**: Collection name (`rag_embedding`)

**Output**: None (side effect: collection exists)

**Logic**:
```python
def create_collection(qdrant_client, collection_name: str = "rag_embedding"):
    from qdrant_client.models import Distance, VectorParams

    # Check if collection exists
    collections = qdrant_client.get_collections().collections
    if any(c.name == collection_name for c in collections):
        return  # Already exists

    # Create with Cohere embed-english-v3.0 config
    qdrant_client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(
            size=1024,  # embed-english-v3.0 dimensions
            distance=Distance.COSINE,
        ),
        hnsw_config={
            "m": 16,
            "ef_construct": 100,
        }
    )
```

---

#### 6. `save_chunk_to_qdrant(chunks: list, embeddings: list, metadata: dict)`

**Purpose**: Upsert vectors with metadata to Qdrant collection.

**Input**:
- `chunks`: List of chunk dicts with `id`, `text`, `position`
- `embeddings`: Corresponding embedding vectors
- `metadata`: Shared metadata (`source_url`, `title`, `section`)

**Output**: Number of points upserted

**Logic**:
```python
def save_chunk_to_qdrant(
    qdrant_client,
    chunks: list,
    embeddings: list,
    metadata: dict,
    collection_name: str = "rag_embedding"
) -> int:
    from qdrant_client.models import PointStruct

    points = []
    for chunk, embedding in zip(chunks, embeddings):
        # Compute content hash for idempotency
        content_hash = hashlib.sha256(chunk['text'].encode()).hexdigest()

        # Note: Qdrant ExtendedPointId accepts string IDs. 32-char hex from SHA-256[:32] is valid.
        point = PointStruct(
            id=chunk['id'],  # Deterministic 32-char hex ID (valid Qdrant string ID)
            vector=embedding,
            payload={
                "source_url": metadata['source_url'],
                "title": metadata['title'],
                "section": metadata['section'],
                "chunk_position": chunk['position'],
                "chunk_text": chunk['text'],
                "content_hash": content_hash,
            }
        )
        points.append(point)

    # Upsert for idempotency (overwrites existing)
    qdrant_client.upsert(
        collection_name=collection_name,
        wait=True,
        points=points,
    )

    return len(points)
```

---

#### 7. `main()`

**Purpose**: Orchestrate pipeline execution with logging and reporting.

**Logic**:
```python
def main():
    import os
    import json
    import time
    from dotenv import load_dotenv
    import cohere
    from qdrant_client import QdrantClient

    # Load environment
    load_dotenv()

    # Validate required env vars
    required = ['COHERE_API_KEY', 'QDRANT_URL', 'QDRANT_API_KEY']
    for var in required:
        if not os.getenv(var):
            raise EnvironmentError(f"Missing required env var: {var}")

    # Initialize clients
    co = cohere.Client(os.getenv('COHERE_API_KEY'))
    qdrant = QdrantClient(
        url=os.getenv('QDRANT_URL'),
        api_key=os.getenv('QDRANT_API_KEY'),
    )

    # Stats tracking
    stats = {
        "start_time": time.time(),
        "urls_processed": 0,
        "urls_failed": 0,
        "chunks_created": 0,
        "embeddings_generated": 0,
        "vectors_stored": 0,
        "failed_urls": [],
    }

    BASE_URL = "https://hackathon-i-physical-ai-humanoid-ro-dun.vercel.app"

    # 1. Create collection
    create_collection(qdrant)
    log({"level": "INFO", "stage": "init", "message": "Collection ready"})

    # 2. Discover URLs
    urls = get_all_urls(BASE_URL)
    log({"level": "INFO", "stage": "discovery", "message": f"Found {len(urls)} URLs"})

    # 3. Process each URL
    for url in urls:
        try:
            # Extract text
            title, section, text = extract_text_from_url(url)

            # Chunk text
            chunks = chunk_text(text, url)
            stats["chunks_created"] += len(chunks)

            # Generate embeddings
            chunk_texts = [c['text'] for c in chunks]
            embeddings = embed(chunk_texts, co)
            stats["embeddings_generated"] += len(embeddings)

            # Store in Qdrant
            metadata = {"source_url": url, "title": title, "section": section}
            stored = save_chunk_to_qdrant(qdrant, chunks, embeddings, metadata)
            stats["vectors_stored"] += stored

            stats["urls_processed"] += 1
            log({"level": "INFO", "stage": "process", "url": url, "chunks": len(chunks)})

        except Exception as e:
            stats["urls_failed"] += 1
            stats["failed_urls"].append({"url": url, "error": str(e)})
            log({"level": "ERROR", "stage": "process", "url": url, "error": str(e)})

    # 4. Report
    stats["end_time"] = time.time()
    stats["duration_seconds"] = stats["end_time"] - stats["start_time"]

    print("\n" + "=" * 60)
    print("EXECUTION REPORT")
    print("=" * 60)
    print(json.dumps(stats, indent=2))


def log(entry: dict):
    """JSON structured logging"""
    import json
    from datetime import datetime
    entry["timestamp"] = datetime.utcnow().isoformat() + "Z"
    print(json.dumps(entry))


if __name__ == "__main__":
    main()
```

---

## Environment Setup

### .env.example

```text
# Cohere API (get from https://dashboard.cohere.com/api-keys)
COHERE_API_KEY=your_cohere_api_key_here

# Qdrant Cloud (get from https://cloud.qdrant.io/)
QDRANT_URL=https://your-cluster-id.us-east-1-0.aws.cloud.qdrant.io:6333
QDRANT_API_KEY=your_qdrant_api_key_here
```

---

## Validation Queries

After pipeline execution, verify with:

```python
# Check collection exists
collections = qdrant.get_collections()
assert any(c.name == "rag_embedding" for c in collections.collections)

# Check vectors stored
info = qdrant.get_collection("rag_embedding")
print(f"Total vectors: {info.points_count}")

# Test retrieval
from cohere import Client
co = Client(os.getenv('COHERE_API_KEY'))
query_emb = co.embed(["ROS2 navigation"], input_type="search_query", model="embed-english-v3.0").embeddings[0]

results = qdrant.search(
    collection_name="rag_embedding",
    query_vector=query_emb,
    limit=3,
    with_payload=True
)
for r in results:
    print(f"Score: {r.score:.3f} | URL: {r.payload['source_url']}")
```

---

## Risk Analysis

| Risk | Mitigation |
|------|------------|
| Cohere API rate limits | Exponential backoff (1s, 2s, 4s), batch size 96 |
| Qdrant cluster suspended | Clear error message, link to reactivation docs |
| HTML structure changes | Multiple CSS selector fallbacks |
| Large documents | Chunking handles any size |

---

## Architectural Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Single file | `main.py` | User requirement, simplicity for hackathon |
| Package manager | UV | Fast, modern Python package manager |
| Sync HTTP | `httpx` (sync mode) | Simpler than async for single-threaded pipeline |
| No ORM | Direct Qdrant client | Single collection, no complex queries |
| JSON logging | `json.dumps()` | Future log aggregation compatibility |
| Collection name | `rag_embedding` | User specified |

ðŸ“‹ **Architectural decision detected**: Single-file architecture for embedding pipeline with 6 core functions using UV package manager. Document reasoning and tradeoffs? Run `/sp.adr single-file-pipeline-architecture`

---

## Next Steps

1. Run `/sp.tasks` to generate implementation tasks
2. Create `backend/` directory and initialize with UV
3. Implement `main.py` with all functions
4. Test with deployed Docusaurus URLs
5. Validate retrieval quality
